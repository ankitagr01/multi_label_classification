{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "jupyter nbconvert --to html /content//GFK_AnkitAgr_ML.ipynb"
      ],
      "metadata": {
        "id": "KpNhmNdO69yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VArbYp86rXQM"
      },
      "source": [
        "\n",
        "```\n",
        "# Notebook for *Multi-label Classification* using Multi-layer perceptron (MLP)\n",
        "```\n",
        "\n",
        "Choice of embeddings:  \n",
        "    Bert (Pre-trained)  \n",
        "    word2vec (custom trained)  \n",
        "    (set them in global variables)\n",
        "\n",
        "\n",
        "For multi-label classification:  \n",
        "The model is trained to predict:  \n",
        "item_id, mdm_model_text and mdm_brand_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Av9v1HKqKEf",
        "outputId": "f2c07552-51f9-443c-cffa-db59d6e87b68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "536\n",
            "all_item_ids ['138176095', '101261697', '140544215', '88210952', '112904161', '86804350', '139354017', '121721616', '24493444', '18378494', '122856012', '94920964', '87099837', '82981040', '102285076', '123927094', '136736656', '92361114', '18378561', '18378572', '94403654', '26726405', '80033948', '127901561', '25278162', '19026396', '79720920', '124606451', '104432648', '115519055', '123883561', '132843807', '103340122', '49498188', '81044073', '121626287', '117860560', '143471294', '141351006', '72145121', '119793326', '127487870', '105092835', '111044504', '76957414', '85892464', '104052744', '58328086', '104052322', '92562474', '105090127', '104386541', '79560221', '138543939', '115239599', '108586370', '80508911', '79452476', '54274515', '112306880', '112904599', '125771101', '125751655', '107335209', '85327619', '124632457', '108102591', '87121775', '98774856', '123805814', '119539790', '95336029', '73320960', '96266916', '79429232', '98426893', '124633843', '33336411', '143679284', '48759730', '112072950', '90224571', '104052382', '110711856', '84825813', '91684877', '112078554', '112253780', '104081771', '122777079', '44682227', '106386361', '104958972', '133401572', '133402413', '106790570', '100983612', '123942253', '133940428', '104056609', '105087451', '124267813', '91744264', '106799763', '106793053', '106789564', '58742287', '104137747', '104139800', '82319609', '112913008', '84964133', '104059877', '143678129', '106793966', '114908022', '85406230', '111371103', '100985123', '91685284', '23824473', '106788109', '124254528', '107401777', '104059091', '105203493', '104089176', '98427383', '90239108', '100152179', '64840970', '53254994', '108234268', '112029452', '85693144', '106791342', '104124615', '38599049', '112327036', '97789708', '91684489', '90120905', '104956666', '91721612', '124254625', '122764584', '84962205', '124277139', '107912326', '106305879', '106792953', '101020016', '106791193', '125567129', '83636387', '111288329', '58082329', '122764737', '79191602', '104134867', '140027109', '122764472', '82319245', '108361704', '112073583', '124691773', '119795873', '79584403', '122776005', '124336576', '140026718', '107913186', '107338251', '103505320', '125558054', '122769301', '104141158', '111371988', '85511674', '2296911', '86061799', '81343196', '125581520', '81575722', '84936238', '125542311', '82064153', '98694005', '125580066', '91907300', '73617866', '112312910', '104055823', '76115381', '104119874', '115607568', '116474836', '143254185', '79636374', '125938763', '84787688', '110734715', '84494388', '28965410', '95958791', '106792878', '104114252', '79713205', '123932889', '103695349', '126087340', '124721429', '26714969', '18378781', '92664252', '80376931', '25110766', '103367065', '142336314', '35296740', '123883610', '123902286', '140524677', '49494636', '103662039', '82928081', '98751799', '133953695', '139669159', '86023305', '124606420', '25714995', '111899848', '82201765', '30524469', '82190140', '56710362', '49935971', '124545319', '124870656', '81533400', '114944228', '89032142', '122786401', '144314284', '96654323', '101240896', '140451774', '140451714', '111920544', '87999683', '133047472', '124834949', '97815698', '97815771', '100444198', '79445735', '144542015', '144542026', '133951720']\n",
            "length all_item_ids 260\n",
            "2868\n",
            "      item_id  ...                                        new_item_id\n",
            "0   138176095  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "5   101261697  ...  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "7   140544215  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "9   138176095  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "21  138176095  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "\n",
            "[5 rows x 6 columns]\n",
            "total unique entries: 2868\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "import unicodedata\n",
        "from gensim.models import Word2Vec\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "from pandas._libs.hashtable import value_count\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from pandas._libs.hashtable import value_count\n",
        "\n",
        "mlbe = preprocessing.MultiLabelBinarizer()\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "tqdm.pandas()\n",
        "\n",
        "# seed for reproducibility\n",
        "seed = 0\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
        "np.random.seed(seed)  # Numpy module.\n",
        "random.seed(seed)  # Python random module.\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# global variables/flags\n",
        "BALANCED_SAMPLING = False\n",
        "BALANCED_SAMPLING_ATMOST = 25\n",
        "WORD2VEC_EMBEDDING_DIMENSION = 100 # word2vec\n",
        "CREATE_ONEHOT_COUNTRY = False \n",
        "# choose the embeddings type from below\n",
        "APPLY_WORD2VEC_EMBEDDINGS = False\n",
        "APPLY_BERT_EMBEDDINGS = True\n",
        "\n",
        "\n",
        "# loading csv file into dataframe using utf-8 encoding\n",
        "orig_df = pd.read_csv('train_lenses_ds_task.csv', encoding='utf-8')\n",
        "\n",
        "# NOTE: if read_csv gives encoding error, open the csv file in sublime or other \n",
        "# text editor and save it with utf-8 encoding and try again\n",
        "\n",
        "df = orig_df.drop_duplicates(keep='first') # drop dupliactes\n",
        "df = df.dropna() # drop NaN\n",
        "df = df.apply(lambda x: x.astype(str).str.upper()) # all string to upper case\n",
        "\n",
        "# balanced sampling take atmost-15 entries for each count\n",
        "if BALANCED_SAMPLING:\n",
        "  df_high = df[df.groupby('item_id').transform('count').ge(BALANCED_SAMPLING_ATMOST)['main_text']]\n",
        "  df_low = df[df.groupby('item_id').transform('count').lt(BALANCED_SAMPLING_ATMOST)['main_text']]\n",
        "  df_high = df_high.groupby('item_id', group_keys=False).apply(lambda x: x.sample(BALANCED_SAMPLING_ATMOST))\n",
        "  df = pd.concat([df_low, df_high], axis=0)\n",
        "\n",
        "\n",
        "# Multilabel: extending classes\n",
        "classes = []\n",
        "classes.extend(pd.unique(df['item_id']))\n",
        "classes.extend(pd.unique(df['mdm_brand_text']))\n",
        "classes.extend(pd.unique(df['mdm_model_text']))\n",
        "print(len(classes))\n",
        "output_layer = len(classes)\n",
        "\n",
        "# list of unique values for each category\n",
        "all_item_ids = pd.unique(df['item_id']).tolist()\n",
        "all_mdm_brand_text = pd.unique(df['mdm_brand_text']).tolist()\n",
        "all_mdm_model_text = pd.unique(df['mdm_model_text']).tolist()\n",
        "print('all_item_ids', all_item_ids)\n",
        "print('length all_item_ids', len(all_item_ids))\n",
        "\n",
        "# will be used later to map from mdm predictions to item_id\n",
        "df['mdm_combine'] = df['mdm_brand_text'] + ' ' + df['mdm_model_text']\n",
        "\n",
        "# Transforming to multi-label class Encoding\n",
        "multilabelclasses = []\n",
        "for ind, row in df.iterrows():\n",
        "  local_list = []\n",
        "  local_list.extend([row['item_id'], row['mdm_brand_text'], row['mdm_model_text']])\n",
        "  multilabelclasses.append(local_list)\n",
        "\n",
        "y_label = mlbe.fit_transform(multilabelclasses)\n",
        "print(len(y_label))\n",
        "df['new_item_id'] = y_label.tolist()\n",
        "\n",
        "# drop below columns as alreadt appended as new classes in new_item_id\n",
        "df = df.drop(columns=['mdm_brand_text', 'mdm_model_text'], axis=1)\n",
        "\n",
        "print(df.head())\n",
        "print('total unique entries:', len(df)) # 2868\n",
        "\n",
        "# creating one-hot encoding for categorical varaible country_name\n",
        "if CREATE_ONEHOT_COUNTRY:\n",
        "  df_categorical = pd.get_dummies(df.country_name, prefix='country')\n",
        "  df = pd.concat([df, df_categorical], axis=1)\n",
        "  df = df.drop('country_name', axis=1)\n",
        "\n",
        "# df.head(30)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5htM64ENF4G"
      },
      "source": [
        "Total unique entries in the data: 2868\n",
        "\n",
        "Total different item_ids: 260  \n",
        "Total different mdm_brand_text: 23  \n",
        "Total different mdm_model_text: 253  \n",
        "Final classes length:  536  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzdcZjMsVHEP"
      },
      "outputs": [],
      "source": [
        "# code for visualization of class distribution \n",
        "# df2 = df.groupby('item_id').count().reset_index()[['item_id', 'main_text']]\n",
        "# df2 = df2.sort_values('main_text', ascending=False)\n",
        "# df2 = df2[df2['main_text']<500]\n",
        "\n",
        "# df2.plot(kind='bar', x='item_id', figsize=(20,10))\n",
        "# plt.xticks(rotation='vertical')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlhMWd-PWheB",
        "outputId": "661ef08b-ebb5-48e3-9c4e-758857140ed8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2294\n",
            "574\n"
          ]
        }
      ],
      "source": [
        "# making train test split\n",
        "# train split = 80%\n",
        "# test split = 20%\n",
        "\n",
        "train, test = train_test_split(df, test_size=0.2, random_state=12)\n",
        "\n",
        "print(len(train)) #2294\n",
        "print(len(test)) #574"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMzdcT1bVjFf"
      },
      "outputs": [],
      "source": [
        "# preprocessing/cleaning of data\n",
        "\n",
        "word2vec_processed_array = []\n",
        "vocabulary = []\n",
        "\n",
        "def strip_accents(text):\n",
        "  \"\"\"\n",
        "    Strip accents from input String.\n",
        "\n",
        "    :param text: The input string.\n",
        "    :type text: String.\n",
        "\n",
        "    :returns: The processed String.\n",
        "    :rtype: String.\n",
        "    \"\"\"\n",
        "  text = unicodedata.normalize('NFD', text)\n",
        "  text = text.encode('ascii', 'ignore')\n",
        "  text = text.decode(\"utf-8\")\n",
        "  return str(text)\n",
        "\n",
        "\n",
        "def preprocess(text, is_train):\n",
        "  \"\"\"\n",
        "    Instead of tokenizing directly removing all punctuations, here\n",
        "    we handle numerical data first, and remove all other special characters.\n",
        "    \n",
        "    Replace all numberical data seperator from (',' or '-' or ',') to '_'\n",
        "    Example:\n",
        "    02.25 = 02_25\n",
        "    02,25 = 02_25\n",
        "    02-25 = 02_25\n",
        "    2.2 = 2_2\n",
        "    \n",
        "    Note: must contain number both side of the special_character\n",
        "    This step is done to maintain the relationship that numerical data may have\n",
        "\n",
        "    Then we replace all special characters and punctuations except '_' with ' '\n",
        "    \n",
        "    Note: we also build the vocabulary of sentences found only in training data\n",
        "    (is_train) = True, so that we can train custom word2vec model for embeddings\n",
        "    \n",
        "    :param text: The input string.\n",
        "    :type text: String.\n",
        "\n",
        "    :param is_train: build vocabulary is is_train = True\n",
        "    :type text: Boolean.\n",
        "\n",
        "    :returns: The processed String.\n",
        "    :rtype: String.\n",
        "    \"\"\"\n",
        "    \n",
        "  processed_text = re.sub('([0-9]+)[,|.|-]([0-9]+)', '\\\\1_\\\\2', text)\n",
        "  processed_text = re.sub('\\W+',' ', processed_text)\n",
        "  processed_text = processed_text.strip()\n",
        "\n",
        "  processed_text = strip_accents(processed_text)\n",
        "  # sent = ' '.join(word for word in processed_text.split() if len(word)>1)\n",
        "\n",
        "  if is_train: # as embedding model will be trained only with training data\n",
        "    processed_words = processed_text.split()\n",
        "    word2vec_processed_array.append(processed_words)\n",
        "    vocabulary.extend(processed_words)\n",
        "  \n",
        "  return processed_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LM1Ijek0VKkW"
      },
      "outputs": [],
      "source": [
        "def preprocess_main(data, is_train=False):\n",
        "  \"\"\"\n",
        "    Main function to preprocess both train and test data\n",
        "    We combine 'main_text' and 'retailer_pg' and 'country_name' \n",
        "    and store it to 'feature_text' column in the dataframe.\n",
        "    And pass them to preprocess method to clean the text\n",
        "    'main_text','retailer_pg','country_name' is then dropped from the dataframe\n",
        "\n",
        "    :param data: Dataframe of train or test split\n",
        "    :type data: Dataframe.\n",
        "    :param is_train: build vocabulary is is_train = True\n",
        "    :type text: Boolean.\n",
        "\n",
        "    :returns: Dataframe \n",
        "    :rtype: String.\n",
        "    \"\"\"\n",
        "  \n",
        "  for index, row in data.iterrows():\n",
        "    text = row['main_text'] + ' ' + row['retailer_pg'] + ' ' + row['country_name']\n",
        "    # get clean processed text from preprocess method\n",
        "    clean_text = preprocess(text, is_train)\n",
        "    # assign clean text to new column feature_text\n",
        "    data.loc[index, 'feature_text'] = clean_text\n",
        "\n",
        "  data = data.drop(columns=['main_text', 'retailer_pg', 'country_name'], axis=1)\n",
        "  return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Rq9l_HQY_s8",
        "outputId": "2a54ea74-7080-4250-85ac-241132968b0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2294, 536)\n",
            "(574, 536)\n"
          ]
        }
      ],
      "source": [
        "# pass train and test splits(dataframe) to preprocess method\n",
        "train = preprocess_main(train, is_train=True)\n",
        "test = preprocess_main(test)\n",
        "\n",
        "# seperate train label Y train\n",
        "y_train = train.pop('new_item_id')\n",
        "y_train = np.array(y_train.values.tolist())\n",
        "print(y_train.shape)\n",
        "\n",
        "# seperate test label Y test:\n",
        "y_test = test.pop('new_item_id')\n",
        "y_test = np.array(y_test.values.tolist())\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Custom Word2Vec\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1mgSt6siSLSd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LU60w-UP9Qka"
      },
      "outputs": [],
      "source": [
        "# Train custom word2Vec model from Gensim\n",
        "# Sentences are processed sentences('main_text' + 'retailer_pg')\n",
        "# sentences only involve training data.\n",
        "\n",
        "if APPLY_WORD2VEC_EMBEDDINGS:\n",
        "  start_time = time.time()\n",
        "\n",
        "  word2vec_model = Word2Vec(sentences=word2vec_processed_array,\n",
        "                  sg=1, # 1 for skip_gram, 0 for CBOW\n",
        "                  min_count=1,\n",
        "                  size=WORD2VEC_EMBEDDING_DIMENSION,  \n",
        "                  workers=4)\n",
        "\n",
        "  print(f'Time taken : {(time.time() - start_time) / 60:.2f} mins')\n",
        "\n",
        "  keys = set(word2vec_model.wv.vocab.keys())\n",
        "  print('Total vocab of trained word2vec model:', len(keys))\n",
        "  print('Dimension of trained word2vec vectors:',word2vec_model.wv.vector_size)\n",
        "  # print(model.wv.get_vector('CRISTALES')) \n",
        "\n",
        "  # Save the word2vec model\n",
        "  word2vec_model.wv.save_word2vec_format('custom_word2vec_model_100d.txt')\n",
        "  word_vectors = word2vec_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSo_-QSTabFZ"
      },
      "outputs": [],
      "source": [
        "def apply_word2vec(data):\n",
        "  \"\"\"\n",
        "  For each data row, take embedding vector for each token in the sentence\n",
        "  from the trained word2vec model \n",
        "  and take mean of those vectors to for one vector for a sentence. \n",
        "  \"\"\"\n",
        "\n",
        "  D = WORD2VEC_EMBEDDING_DIMENSION\n",
        "\n",
        "  X = np.zeros((len(data), D))\n",
        "  n = 0\n",
        "  emptycount = 0\n",
        "  for rows in data:\n",
        "    tokens = rows.split()\n",
        "    vecs = []\n",
        "    m = 0\n",
        "    for word in tokens:\n",
        "      try:\n",
        "        # throws KeyError if word not found\n",
        "        vec = word_vectors.wv.get_vector(word)\n",
        "        vecs.append(vec)\n",
        "        m += 1\n",
        "      except KeyError:\n",
        "        pass\n",
        "    if len(vecs) > 0:\n",
        "      vecs = np.array(vecs)\n",
        "      X[n] = vecs.mean(axis=0)\n",
        "    else:\n",
        "      emptycount += 1\n",
        "    n += 1\n",
        "  print(\"Numer of samples with no words found: %s / %s\" % (emptycount, len(data)))\n",
        "  return X\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bert Embedding:"
      ],
      "metadata": {
        "id": "NNiGUKURO2Yl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if APPLY_BERT_EMBEDDINGS:\n",
        "  !pip install -U sentence-transformers\n",
        "  from sentence_transformers import SentenceTransformer\n",
        "\n",
        "  bert_model = SentenceTransformer('sentence-transformers/bert-base-nli-mean-tokens')\n",
        "\n",
        "def get_bert_embeddings(data):\n",
        "  '''\n",
        "  Apply bert embeddings to sentences provided in dataframe\n",
        "  :param data: Series of sentences\n",
        "  :type data: Dataframe Series\n",
        "\n",
        "  :returns: list of embeddings of sentences \n",
        "  :shape: (num_of_sentences, bert_embd_dimension)\n",
        "  :rtype: List of List.\n",
        "  '''\n",
        "  D = 768\n",
        "  data = data.to_list()\n",
        "  embeddings = bert_model.encode(data)\n",
        "  return embeddings\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2SOu0pEO5mv",
        "outputId": "5670c437-d020-40d7-9755-29e49872885e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.2.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.10.0+cu111)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.16.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.11.1+cu111)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.96)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.62.3)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.10.0.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.47)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.11.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.11.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.10.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " \n",
        "For Bert embeddings:  \n",
        "We directly ger embeddings of the complete sentence\n"
      ],
      "metadata": {
        "id": "oUWaebSQSuD5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSlhreJmh-j3",
        "outputId": "0ccad188-974c-453a-ad5b-7f12ddc60ac5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2294, 768)\n",
            "(574, 768)\n"
          ]
        }
      ],
      "source": [
        "# Generating embeddings of train/test using Bert\n",
        "\n",
        "if APPLY_BERT_EMBEDDINGS:\n",
        "  # Train:\n",
        "  # Get the features(embedding vectors) for train data\n",
        "  x_train = get_bert_embeddings(train['feature_text'])\n",
        "  print(x_train.shape)\n",
        "\n",
        "  # Test:\n",
        "  # Get the features(embedding vectors) for test data\n",
        "  x_test = get_bert_embeddings(test['feature_text'])\n",
        "  print(x_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For word2vec embeddings:   \n",
        "We can get the embeddings of complete sentence in feature_text by taking mean of all embedding vectors of its words."
      ],
      "metadata": {
        "id": "C6sF6dlAS1w3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating embeddings of train/test using word2vec\n",
        "\n",
        "if APPLY_WORD2VEC_EMBEDDINGS:\n",
        "  # Train:\n",
        "  # Get the features(embedding vectors) for train data\n",
        "  x_train = apply_word2vec(train['feature_text'])\n",
        "  print(x_train.shape)\n",
        "\n",
        "  if CREATE_ONEHOT_COUNTRY:\n",
        "    # Add features from one-hot encoding of country\n",
        "    country_train = train[['country_GERMANY', 'country_SPAIN']].to_numpy()\n",
        "    print(country_train.shape)\n",
        "\n",
        "    x_train = np.concatenate((x_train, country_train), axis=1)\n",
        "    print(x_train.shape)\n",
        "\n",
        "  # Test:\n",
        "  # Get the features(embedding vectors) for test data\n",
        "  x_test = apply_word2vec(test['feature_text'])\n",
        "  print(x_test.shape)\n",
        "\n",
        "  if CREATE_ONEHOT_COUNTRY:\n",
        "    # Add features from one-hot encoding of country\n",
        "    country_test = test[['country_GERMANY', 'country_SPAIN']].to_numpy()\n",
        "    print(country_test.shape)\n",
        "\n",
        "    x_test = np.concatenate((x_test, country_test), axis=1)\n",
        "    print(x_test.shape)"
      ],
      "metadata": {
        "id": "rjO-HFLdPDI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAbhLRF2hWID"
      },
      "source": [
        "## Deep Learning Multi-Label Classifier\n",
        "\n",
        "We train a neural network MLP model to classify multi-label i.e. to predict ('item_id', 'mdm_brand_text', 'mdm_model_text')\n",
        "\n",
        "Currently for some data points, the prediction features ('main_text, 'retailer_pg', 'country_name')\n",
        "are same but the classes(item_id) are different. which depends on 'mdm_brand_text', 'mdm_model_text'\n",
        "And as these two columns are not available during prediction, it is hard for the model to return the correct item_id in this ambiguous senario. \n",
        "\n",
        "So we train the model to predict multi-label, all ('item_id', 'mdm_brand_text', 'mdm_model_text'), so that while learning the model to predict all three it learns to distinguish the case above.\n",
        "\n",
        "Implementation:  \n",
        "In test time accuracy, We only compare the item_id as other fields are not available at prediction time.  \n",
        "\n",
        "We find three accuracy metric here:\n",
        "1. For item_id directly predicted by the model ()\n",
        "2. For item_id generated using the mapping from predicted value of mdm_brand_text and mdm_model_text\n",
        "3. Multi-label accuracy (This is not the correct metric as it uses mdm_brand and mdm_model data at prediction to get the accuracy.)\n",
        "\n",
        "Parameters of the network:  \n",
        "\n",
        "if using Word2vec embeddings:  \n",
        "1. Input Dimension: 102\n",
        "2. Hidden Layer 1: 512\n",
        "3. Hidden Layer 2: 1024\n",
        "3. Output Layer 536 \n",
        "\n",
        "if using Word2vec embeddings:  \n",
        "1. Input Dimension: 768\n",
        "2. Hidden Layer 1: 1024\n",
        "3. Hidden Layer 2: 512\n",
        "3. Output Layer 536 \n",
        "\n",
        "\n",
        "Label:  \n",
        "(combining all unique item_id, mdm_brand, mdm_model) and creating 1-hot encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8BwmKTnaccK",
        "outputId": "f92c0924-0b23-44fb-fd0e-89e8d449d11b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using: cuda device\n",
            "epoch value =: 50\n",
            "Batch Size =: 8\n",
            "Accuracy: {'automatic_item_id_accuracy': 0.7979094076655052, 'calculated_item_id_accuracy': 0.8031358885017421, 'multilabel_accuracy': tensor(0.8216)}\n",
            "epoch value =: 50\n",
            "Batch Size =: 12\n",
            "Accuracy: {'automatic_item_id_accuracy': 0.8013937282229965, 'calculated_item_id_accuracy': 0.8083623693379791, 'multilabel_accuracy': tensor(0.8221)}\n",
            "epoch value =: 50\n",
            "Batch Size =: 32\n",
            "Accuracy: {'automatic_item_id_accuracy': 0.8310104529616724, 'calculated_item_id_accuracy': 0.8257839721254355, 'multilabel_accuracy': tensor(0.8486)}\n",
            "epoch value =: 50\n",
            "Batch Size =: 64\n",
            "Accuracy: {'automatic_item_id_accuracy': 0.8083623693379791, 'calculated_item_id_accuracy': 0.8118466898954704, 'multilabel_accuracy': tensor(0.8348)}\n",
            "epoch value =: 50\n",
            "Batch Size =: 128\n",
            "Accuracy: {'automatic_item_id_accuracy': 0.7700348432055749, 'calculated_item_id_accuracy': 0.7839721254355401, 'multilabel_accuracy': tensor(0.7951)}\n",
            "epoch value =: 100\n",
            "Batch Size =: 8\n",
            "Accuracy: {'automatic_item_id_accuracy': 0.8066202090592335, 'calculated_item_id_accuracy': 0.813588850174216, 'multilabel_accuracy': tensor(0.8277)}\n",
            "epoch value =: 100\n",
            "Batch Size =: 12\n",
            "Accuracy: {'automatic_item_id_accuracy': 0.8449477351916377, 'calculated_item_id_accuracy': 0.8432055749128919, 'multilabel_accuracy': tensor(0.8585)}\n",
            "epoch value =: 100\n",
            "Batch Size =: 32\n",
            "Accuracy: {'automatic_item_id_accuracy': 0.8501742160278746, 'calculated_item_id_accuracy': 0.8466898954703833, 'multilabel_accuracy': tensor(0.8636)}\n",
            "epoch value =: 100\n",
            "Batch Size =: 64\n",
            "Accuracy: {'automatic_item_id_accuracy': 0.8344947735191638, 'calculated_item_id_accuracy': 0.8362369337979094, 'multilabel_accuracy': tensor(0.8577)}\n",
            "epoch value =: 100\n",
            "Batch Size =: 128\n",
            "Accuracy: {'automatic_item_id_accuracy': 0.8275261324041812, 'calculated_item_id_accuracy': 0.8292682926829268, 'multilabel_accuracy': tensor(0.8389)}\n",
            "epoch value =: 150\n",
            "Batch Size =: 8\n",
            "Accuracy: {'automatic_item_id_accuracy': 0.8414634146341463, 'calculated_item_id_accuracy': 0.837979094076655, 'multilabel_accuracy': tensor(0.8591)}\n",
            "epoch value =: 150\n",
            "Batch Size =: 12\n",
            "Accuracy: {'automatic_item_id_accuracy': 0.8031358885017421, 'calculated_item_id_accuracy': 0.8153310104529616, 'multilabel_accuracy': tensor(0.8279)}\n",
            "epoch value =: 150\n",
            "Batch Size =: 32\n",
            "Accuracy: {'automatic_item_id_accuracy': 0.8449477351916377, 'calculated_item_id_accuracy': 0.8536585365853658, 'multilabel_accuracy': tensor(0.8693)}\n",
            "epoch value =: 150\n",
            "Batch Size =: 64\n",
            "Accuracy: {'automatic_item_id_accuracy': 0.8240418118466899, 'calculated_item_id_accuracy': 0.8327526132404182, 'multilabel_accuracy': tensor(0.8446)}\n",
            "epoch value =: 150\n",
            "Batch Size =: 128\n",
            "Accuracy: {'automatic_item_id_accuracy': 0.8170731707317073, 'calculated_item_id_accuracy': 0.818815331010453, 'multilabel_accuracy': tensor(0.8395)}\n",
            "epoch value =: 200\n",
            "Batch Size =: 8\n",
            "Accuracy: {'automatic_item_id_accuracy': 0.8205574912891986, 'calculated_item_id_accuracy': 0.8257839721254355, 'multilabel_accuracy': tensor(0.8453)}\n",
            "epoch value =: 200\n",
            "Batch Size =: 12\n",
            "Accuracy: {'automatic_item_id_accuracy': 0.8414634146341463, 'calculated_item_id_accuracy': 0.8432055749128919, 'multilabel_accuracy': tensor(0.8618)}\n",
            "epoch value =: 200\n",
            "Batch Size =: 32\n",
            "Accuracy: {'automatic_item_id_accuracy': 0.8031358885017421, 'calculated_item_id_accuracy': 0.8083623693379791, 'multilabel_accuracy': tensor(0.8230)}\n",
            "epoch value =: 200\n",
            "Batch Size =: 64\n",
            "Accuracy: {'automatic_item_id_accuracy': 0.8484320557491289, 'calculated_item_id_accuracy': 0.8466898954703833, 'multilabel_accuracy': tensor(0.8632)}\n",
            "epoch value =: 200\n",
            "Batch Size =: 128\n",
            "Accuracy: {'automatic_item_id_accuracy': 0.8240418118466899, 'calculated_item_id_accuracy': 0.8292682926829268, 'multilabel_accuracy': tensor(0.8512)}\n",
            "epoch value =: 300\n",
            "Batch Size =: 32\n",
            "Accuracy: {'automatic_item_id_accuracy': 0.8397212543554007, 'calculated_item_id_accuracy': 0.8344947735191638, 'multilabel_accuracy': tensor(0.8557)}\n",
            "epoch value =: 300\n",
            "Batch Size =: 64\n",
            "Accuracy: {'automatic_item_id_accuracy': 0.8554006968641115, 'calculated_item_id_accuracy': 0.8501742160278746, 'multilabel_accuracy': tensor(0.8697)}\n",
            "epoch value =: 300\n",
            "Batch Size =: 128\n",
            "Accuracy: {'automatic_item_id_accuracy': 0.8397212543554007, 'calculated_item_id_accuracy': 0.8414634146341463, 'multilabel_accuracy': tensor(0.8599)}\n",
            "epoch value =: 400\n",
            "Batch Size =: 32\n",
            "Accuracy: {'automatic_item_id_accuracy': 0.8414634146341463, 'calculated_item_id_accuracy': 0.8397212543554007, 'multilabel_accuracy': tensor(0.8646)}\n",
            "epoch value =: 400\n",
            "Batch Size =: 64\n",
            "Accuracy: {'automatic_item_id_accuracy': 0.8449477351916377, 'calculated_item_id_accuracy': 0.8484320557491289, 'multilabel_accuracy': tensor(0.8653)}\n",
            "epoch value =: 400\n",
            "Batch Size =: 128\n",
            "Accuracy: {'automatic_item_id_accuracy': 0.8554006968641115, 'calculated_item_id_accuracy': 0.8519163763066202, 'multilabel_accuracy': tensor(0.8693)}\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "# seed for reproducibility\n",
        "seed = 0\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
        "np.random.seed(seed)  # Numpy module.\n",
        "random.seed(seed)  # Python random module.\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using: {device} device\")\n",
        "\n",
        "class AnkitDataset(Dataset):\n",
        "    def __init__(self, feature_list, target_list):\n",
        "        self.feature_list = feature_list\n",
        "        self.target_list = target_list\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        feature = self.feature_list[index]\n",
        "        target = self.target_list[index]\n",
        "\n",
        "        return feature, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.feature_list)\n",
        "\n",
        "\n",
        "class AnkitNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    if APPLY_BERT_EMBEDDINGS:\n",
        "      self.fc1 = nn.Linear(768, 1024, bias=False)\n",
        "      self.relu = nn.ReLU()\n",
        "      self.fc2 = nn.Linear(1024, 768, bias=False)\n",
        "      self.fc3 = nn.Linear(768, output_layer, bias=False)\n",
        "\n",
        "    if APPLY_WORD2VEC_EMBEDDINGS:\n",
        "      self.fc1 = nn.Linear(100, 512, bias=False)\n",
        "      self.relu = nn.ReLU()\n",
        "      self.fc2 = nn.Linear(512, 1024, bias=False)\n",
        "      self.fc3 = nn.Linear(1024, output_layer, bias=False)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.fc1(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.fc2(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.fc3(out)\n",
        "    return out\n",
        "\n",
        "\n",
        "def Accuracy(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  This function calculates 3 types of accuracy:\n",
        "  1. Overall Accuracy for item_id predicted by multi-label model\n",
        "  2. Overall Accuracy for item_id converted from predicted mdm_model and mdm_brand by multi-label model\n",
        "  3. Multi-label overall accuracy (This metric will not be valid as it takes mdm values in prediction time)\n",
        "  \n",
        "  for 1:\n",
        "    we directly take the item_id predicted by model and compare to y_label\n",
        "\n",
        "  for 2:\n",
        "    we take mdm_model and mdm_brand values predicted by model, \n",
        "    map it to item_id available and compare to y_label\n",
        "\n",
        "  NOTE: we dont use mdm_model and mdm_brand in prediction time\n",
        "  \"\"\"\n",
        "  # final accuravy dict\n",
        "  accuracy_results = {}\n",
        "  # converting output with sigmoid\n",
        "  y_pred = torch.sigmoid(y_pred)\n",
        "\n",
        "  # we take top 3 classes out of 536 \n",
        "  topkvalues, indices = torch.topk(y_pred, k=3, dim=1)\n",
        "  top_third_values, _ = torch.min(topkvalues, dim=1)\n",
        "\n",
        "  # get top 3 classes for each row\n",
        "  for i in range(y_true.shape[0]): \n",
        "    y_pred[i] = y_pred[i]>=top_third_values[i] \n",
        "\n",
        "\n",
        "  top_third_values.detach().cpu()\n",
        "  temp = 0\n",
        "  automatic_match_count = 0\n",
        "  calculated_match_count = 0\n",
        "\n",
        "  y_true = y_true.detach().cpu()\n",
        "  y_pred = y_pred.detach().cpu()\n",
        "\n",
        "  # transforming back from multilabel encoder to original classes\n",
        "  true_classes = mlbe.inverse_transform(y_true)\n",
        "  pred_classes = mlbe.inverse_transform(y_pred)\n",
        "\n",
        "  for i in range(y_true.shape[0]):\n",
        "   \n",
        "    # from true label at prediction time we can only have item_id.\n",
        "    true_item_id = set(true_classes[i]).intersection(set(all_item_ids))\n",
        "\n",
        "    # item id predicted by model directly\n",
        "    pred_item_ids = set(pred_classes[i]).intersection(set(all_item_ids)) #automatic\n",
        "\n",
        "    # below extract mdm_model and mdm_brand predicted by model\n",
        "    pred_mdm_brand_texts = set(pred_classes[i]).intersection(set(all_mdm_brand_text))\n",
        "    pred_mdm_model_texts = set(pred_classes[i]).intersection(set(all_mdm_model_text))\n",
        "    pred_mdm_brand_text = pred_mdm_brand_texts.pop() if len(pred_mdm_brand_texts) >=1 else None\n",
        "    pred_mdm_model_text = pred_mdm_model_texts.pop() if len(pred_mdm_model_texts) >=1 else None\n",
        "    \n",
        "    # from mdm_model mdm_brand map it to item_id (converted item_id)\n",
        "    if pred_mdm_brand_text and pred_mdm_model_text:\n",
        "      search_text = pred_mdm_brand_text + ' ' + pred_mdm_model_text\n",
        "      try:\n",
        "        converted_pred_item_id = set()\n",
        "        converted_pred_item_id_ = df.loc[df['mdm_combine'] == search_text, 'item_id'].iloc[0]\n",
        "        converted_pred_item_id_ = str(converted_pred_item_id_)\n",
        "        converted_pred_item_id.add(converted_pred_item_id_)\n",
        "      except:\n",
        "        converted_pred_item_id = pred_item_ids  \n",
        "    else:\n",
        "      converted_pred_item_id = pred_item_ids\n",
        "    \n",
        "\n",
        "    if true_item_id.intersection(pred_item_ids):\n",
        "      automatic_match_count += 1\n",
        "      \n",
        "    if true_item_id.intersection(converted_pred_item_id):\n",
        "      calculated_match_count += 1\n",
        "\n",
        "    temp += sum(np.logical_and(y_true[i], y_pred[i])) / sum(np.logical_or(y_true[i], y_pred[i]))\n",
        "\n",
        "  accuracy_results = {'automatic_item_id_accuracy': automatic_match_count/y_true.shape[0],\n",
        "                      'calculated_item_id_accuracy': calculated_match_count/y_true.shape[0],\n",
        "                      'multilabel_accuracy': (temp / y_true.shape[0])}\n",
        "  return accuracy_results\n",
        "  \n",
        "\n",
        "def train(model, train_loader, criterion, optimizer, epochs):\n",
        "  model.train()\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    losses = []\n",
        "    for idx, (data, label) in enumerate(train_loader):\n",
        "      data = data.float().to(device)\n",
        "      label = label.float().to(device)\n",
        "\n",
        "      model.to(device)\n",
        "      inputv = data\n",
        "      labelsv = label\n",
        "      \n",
        "      output = model(inputv)\n",
        "      loss = criterion(output, labelsv)\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    losses.append(loss.item())\n",
        "  return model\n",
        "\n",
        "def test(model, test_loader):\n",
        "  model.eval()\n",
        "  gt = []\n",
        "  pred = []\n",
        "\n",
        "  for idx, (data, label) in enumerate(test_loader):\n",
        "    data = data.float().to(device)\n",
        "    label = label.float().to(device)\n",
        "\n",
        "    inputv = Variable(data)\n",
        "    labelsv = Variable(label)\n",
        "    \n",
        "    output = model(inputv)\n",
        "    gt.append(labelsv)\n",
        "    pred.append(output)\n",
        "  \n",
        "  gt = torch.cat(gt, dim=0)\n",
        "  pred = torch.cat(pred, dim=0)\n",
        "  return Accuracy(gt, pred)\n",
        "\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "\n",
        "def ankit_trainer(train_input:list=None, train_target:list=None, \n",
        "                  test_input:list=None, test_target:list=None, \n",
        "                  epoch:int=200, bs:int=16, lr:float=1e-3):\n",
        "\n",
        "  train_dataset = AnkitDataset(np.array(train_input), np.array(train_target))\n",
        "  test_dataset = AnkitDataset(np.array(test_input), np.array(test_target))\n",
        "\n",
        "  # train_dl = DataLoader(train_dataset, batch_size = bs, shuffle = True, num_workers=0, worker_init_fn=seed_worker)\n",
        "  # test_dl = DataLoader(test_dataset, batch_size = 16, shuffle = False, num_workers=0, worker_init_fn=seed_worker)\n",
        "  train_dl = DataLoader(train_dataset, batch_size = bs, shuffle = True, num_workers=0)\n",
        "  test_dl = DataLoader(test_dataset, batch_size = 16, shuffle = False, num_workers=0)\n",
        "\n",
        "\n",
        "  model = AnkitNet()\n",
        "\n",
        "  optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "  # criterion = nn.MultiLabelSoftMarginLoss() \n",
        "  criterion = nn.BCEWithLogitsLoss() \n",
        "\n",
        "  model = train(model, train_dl, criterion, optimizer, epoch)\n",
        "  accuracy_results = test(model, test_dl)\n",
        "  return accuracy_results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  # Hyperparameter search\n",
        "  epochs = [50, 100, 150, 200, 300, 400]\n",
        "  batch_size = [8, 12, 32, 64, 128]\n",
        "  # epochs = [2,2,2]\n",
        "  for epoch in epochs:\n",
        "    for bs in batch_size:\n",
        "      if epoch in (300,400) and bs in (8,12):\n",
        "        continue\n",
        "      print('epoch value =:', epoch)\n",
        "      print('Batch Size =:', bs)\n",
        "      results = ankit_trainer(x_train, y_train, x_test, y_test, epoch, bs)\n",
        "      print('Accuracy:', results)\n",
        "\n",
        "  ## debug\n",
        "  # results = ankit_trainer(x_train, y_train, x_test, y_test, 5, 8)\n",
        "  # print('Accuracy:', results)\n",
        "\n",
        "\n",
        "  "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "GFK_AnkitAgr_MultiLabel.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}